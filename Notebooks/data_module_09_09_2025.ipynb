{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1NrvrZ76lcnshXDI3r9BBswKCiU48tdQZ","authorship_tag":"ABX9TyNzkgdqvADW7ZsTTXK5Ijx3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“Œ Upgrades Made to `DGazeDataset` & `DGazeDataModule`\n","\n","This dataset + datamodule implementation has been upgraded step-by-step to make it **more consistent, faster, and easier to debug/visualize**.  \n","\n","---\n","\n","## ðŸ”¹ 1. Global Normalization\n","- A new method `compute_stats()` computes **mean/std** of **all eye images and facial features** across the entire dataset (train + val + test).\n","- Normalization is now **consistent** across all splits (no train/test mismatch).\n","\n","---\n","\n","## ðŸ”¹ 2. Automatic Stats Handling\n","- If a precomputed `global_stats.pkl` exists â†’ it is **loaded automatically**.  \n","- If not â†’ stats are **computed once**, then **saved for future runs**.  \n","\n","---\n","\n","## ðŸ”¹ 3. Dataset Caching\n","- Added `cache_normalized_dataset()` method.\n","- After dataset creation in `setup()`, the normalized datasets (train/val/test) are **cached into a pickle file**.\n","- Greatly speeds up re-runs in Colab.\n","\n","---\n","\n","## ðŸ”¹ 4. Automatic Cache Loading\n","- If a `normalized_dataset.pkl` cache file exists, it is **loaded directly** inside `prepare_data()`.\n","- `setup()` automatically skips rebuilding datasets if cache is already loaded.\n","\n","---\n","\n","## ðŸ”¹ 5. Dataset Verification\n","- Added `verify_dataset()` method:\n","  - Prints mean/std of normalized eye images.  \n","  - Prints mean/std of normalized facial features.  \n","- Ensures preprocessing is correct.\n","\n","---\n","\n","## ðŸ”¹ 6. Sample Plotting\n","- Added `plot_samples(split=\"train\", num_samples=5)` method:\n","  - Randomly selects samples.  \n","  - Displays their **eye images**.  \n","  - Creates a **scatter plot of gaze points** in normalized `[0,1] Ã— [0,1]` space.  \n","\n","---\n","\n","## ðŸ”¹ 7. Lazy Loading\n","- Instead of preloading everything into memory, the dataset builds an **index of (driver, sequence, frame)**.  \n","- Frames are loaded **on demand** â†’ saves memory.\n","\n","---\n","\n","## ðŸ”¹ 8. Simplified Usage\n","Now the full pipeline is super easy:\n","\n","```python\n","# Initialize DataModule\n","dm = DGazeDataModule(\n","    data_path=\"driver_data.pkl\",\n","    split_path=\"splits.pkl\",\n","    stat_path=\"global_stats.pkl\",          # global stats file\n","    cache_path=\"normalized_dataset.pkl\",   # optional cache file\n","    batch_size=64\n",")\n","\n","# Prepare data\n","dm.prepare_data()    # loads stats & cache automatically\n","dm.setup()           # builds datasets if needed\n","\n","# Verify normalization\n","dm.verify_dataset()\n","\n","# Visualize samples\n","dm.plot_samples(split=\"train\", num_samples=5)\n"],"metadata":{"id":"SrreLSzFAs9M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_v-KKTo3KRa"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","import lightning as L\n","import pickle\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n","\n","class DGazeDataset(Dataset):\n","    def __init__(self, driver_data, drivers, sequences,\n","                 eye_mean, eye_std, feat_mean, feat_std,\n","                 transform=False):\n","        \"\"\"\n","        Hybrid Dataset:\n","        - Lazy loads frames (low memory usage).\n","        - Uses precomputed global normalization stats for eye images & facial features.\n","        - Normalizes gaze points to [0,1].\n","        \"\"\"\n","\n","        self.driver_data = driver_data\n","        self.drivers = drivers\n","        self.sequences = sequences\n","        self.transform = transform\n","\n","        # Save stats directly\n","        self.eye_mean = eye_mean\n","        self.eye_std = eye_std\n","        self.feat_mean = feat_mean\n","        self.feat_std = feat_std\n","\n","        self.index = []  # list of (driver, seq_key, frame_idx)\n","\n","        # Pre-build index list\n","        for driver in drivers:\n","            data = driver_data[driver]\n","            for seq in tqdm(sequences, desc=f\"Indexing driver {driver}\"):\n","                seq_key = f\"seq{seq}\"\n","                if seq_key in data:\n","                    num_frames = len(data[seq_key][\"left_eye\"])\n","                    for frame_idx in range(num_frames):\n","                        self.index.append((driver, seq_key, frame_idx))\n","\n","        print(f\"Lazy dataset ready! Total samples: {len(self.index)}\")\n","\n","    def __len__(self):\n","        return len(self.index)\n","\n","    def __getitem__(self, idx):\n","        driver, seq_key, frame_idx = self.index[idx]\n","        data_seq = self.driver_data[driver][seq_key]\n","\n","        # --- Load & normalize left eye ---\n","        left_eye = data_seq[\"left_eye\"][frame_idx].astype(np.float32)\n","        left_eye = (left_eye - self.eye_mean) / (self.eye_std + 1e-6)\n","        left_eye = np.transpose(left_eye, (2, 0, 1))  # (C,H,W)\n","        left_eye = torch.tensor(left_eye, dtype=torch.float32)\n","\n","        # --- Load & normalize facial features ---\n","        headpose = data_seq[\"headpose_pupil\"][frame_idx, 1:].astype(np.float32)\n","        face_loc = data_seq[\"face_location\"][frame_idx].astype(np.float32)\n","        facial = np.concatenate((headpose, face_loc))\n","        facial = (facial - self.feat_mean) / (self.feat_std + 1e-6)\n","        facial_features = torch.tensor(facial, dtype=torch.float32)\n","\n","        # --- Load gaze point & normalize to [0,1] ---\n","        gaze_point = data_seq[\"gaze_point\"][frame_idx, :2].astype(np.float32).copy()\n","        gaze_point[0] = np.clip(gaze_point[0], 0, 1919) / 1920.0\n","        gaze_point[1] = np.clip(gaze_point[1], 0, 1079) / 1080.0\n","        gaze_point = torch.tensor(gaze_point, dtype=torch.float32)\n","\n","        if self.transform:\n","            # torchvision transforms can go here\n","            pass\n","\n","        return left_eye, facial_features, gaze_point\n"]},{"cell_type":"code","source":["\n","class DGazeDataModule(L.LightningDataModule):\n","    def __init__(self, data_path, split_path, stat_path,\n","                 batch_size=64, num_workers=4, transform=False,\n","                 cache_path=None):\n","        super().__init__()\n","        self.data_path = data_path\n","        self.split_path = split_path\n","        self.stat_path = stat_path\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.transform = transform\n","        self.cache_path = cache_path\n","\n","        self.driver_data = None\n","        self.data_split = None\n","        self.data_stat = None\n","        self.train_dataset = None\n","        self.val_dataset = None\n","        self.test_dataset = None\n","\n","    def prepare_data(self):\n","        # Load driver data and splits\n","        with open(self.data_path, \"rb\") as f:\n","            self.driver_data = pickle.load(f)\n","        with open(self.split_path, \"rb\") as f:\n","            self.data_split = pickle.load(f)\n","\n","        # Load or compute stats\n","        if os.path.exists(self.stat_path):\n","            print(f\"Loading precomputed stats from {self.stat_path}\")\n","            with open(self.stat_path, \"rb\") as f:\n","                self.data_stat = pickle.load(f)\n","        else:\n","            print(\"Computing global stats (first run)...\")\n","            self.data_stat = self.compute_stats()\n","            with open(self.stat_path, \"wb\") as f:\n","                pickle.dump(self.data_stat, f)\n","            print(f\"Saved stats to {self.stat_path}\")\n","\n","        # Load cached normalized dataset if available\n","        if self.cache_path and os.path.exists(self.cache_path):\n","            print(f\"Loading cached normalized dataset from {self.cache_path}\")\n","            with open(self.cache_path, \"rb\") as f:\n","                cached = pickle.load(f)\n","            self.train_dataset, self.val_dataset, self.test_dataset = (\n","                cached[\"train\"], cached[\"val\"], cached[\"test\"]\n","            )\n","\n","    def compute_stats(self):\n","        print(\"Computing global stats (first run)...\")\n","\n","        all_features = []\n","        all_pixels = []\n","\n","        all_drivers = (\n","            list(self.data_split[\"drivers_train\"])\n","            + list(self.data_split[\"drivers_val\"])\n","            + list(self.data_split[\"drivers_test\"])\n","        )\n","        all_sequences = (\n","            list(self.data_split[\"sequence_train\"])\n","            + list(self.data_split[\"sequence_val\"])\n","            + list(self.data_split[\"sequence_test\"])\n","        )\n","\n","        for driver in tqdm(all_drivers, desc=\"Scanning drivers\"):\n","            data = self.driver_data[driver]\n","            for seq in all_sequences:\n","                seq_key = f\"seq{seq}\"\n","                if seq_key not in data:\n","                    continue\n","\n","                num_frames = len(data[seq_key][\"left_eye\"])\n","                for frame_idx in range(num_frames):\n","                    eye_img = data[seq_key][\"left_eye\"][frame_idx].astype(np.float32)\n","                    all_pixels.append(eye_img.reshape(-1, 3))\n","\n","                    headpose = data[seq_key][\"headpose_pupil\"][frame_idx, 1:].astype(np.float32)\n","                    face_loc = data[seq_key][\"face_location\"][frame_idx].astype(np.float32)\n","                    all_features.append(np.concatenate((headpose, face_loc)))\n","\n","        all_pixels = np.vstack(all_pixels)\n","        all_features = np.vstack(all_features)\n","\n","        self.eye_mean = all_pixels.mean(axis=0)\n","        self.eye_std = all_pixels.std(axis=0)\n","\n","        self.feat_mean = all_features.mean(axis=0)\n","        self.feat_std = all_features.std(axis=0)\n","\n","        stats = {\n","            \"eye_mean\": self.eye_mean,\n","            \"eye_std\": self.eye_std,\n","            \"feat_mean\": self.feat_mean,\n","            \"feat_std\": self.feat_std,\n","        }\n","\n","        return stats\n","\n","    def cache_normalized_dataset(self):\n","        \"\"\"Save fully constructed datasets into cache (.pkl).\"\"\"\n","        if self.cache_path is None:\n","            print(\"No cache path provided. Skipping caching.\")\n","            return\n","\n","        print(f\"Caching normalized datasets to {self.cache_path} ...\")\n","        cache_data = {\n","            \"train\": self.train_dataset,\n","            \"val\": self.val_dataset,\n","            \"test\": self.test_dataset,\n","        }\n","        with open(self.cache_path, \"wb\") as f:\n","            pickle.dump(cache_data, f)\n","        print(\"Datasets cached successfully!\")\n","\n","    def setup(self, stage=None):\n","        if self.train_dataset and self.val_dataset and self.test_dataset:\n","            return  # Already loaded from cache\n","\n","        # Extract stats\n","        eye_mean = self.data_stat[\"eye_mean\"]\n","        eye_std = self.data_stat[\"eye_std\"]\n","        feat_mean = self.data_stat[\"feat_mean\"]\n","        feat_std = self.data_stat[\"feat_std\"]\n","\n","        # Build datasets\n","        self.train_dataset = DGazeDataset(\n","            self.driver_data, self.data_split[\"drivers_train\"],\n","            self.data_split[\"sequence_train\"],\n","            eye_mean, eye_std, feat_mean, feat_std,\n","            transform=self.transform\n","        )\n","        self.val_dataset = DGazeDataset(\n","            self.driver_data, self.data_split[\"drivers_val\"],\n","            self.data_split[\"sequence_val\"],\n","            eye_mean, eye_std, feat_mean, feat_std,\n","            transform=self.transform\n","        )\n","        self.test_dataset = DGazeDataset(\n","            self.driver_data, self.data_split[\"drivers_test\"],\n","            self.data_split[\"sequence_test\"],\n","            eye_mean, eye_std, feat_mean, feat_std,\n","            transform=self.transform\n","        )\n","\n","        if self.cache_path:\n","            self.cache_normalized_dataset()\n","\n","    # --- your verification function ---\n","    def verify_dataset(self, num_batches=5, split=\"train\"):\n","        if split == \"train\":\n","            loader = self.train_dataloader()\n","        elif split == \"val\":\n","            loader = self.val_dataloader()\n","        elif split == \"test\":\n","            loader = self.test_dataloader()\n","        else:\n","            raise ValueError(\"split must be 'train', 'val', or 'test'\")\n","\n","        print(f\"Verifying {split} dataset...\")\n","        eye_means, eye_stds, feat_means, feat_stds = [], [], [], []\n","\n","        for i, (left_eye, facial_features, gaze_point) in enumerate(loader):\n","            print(f\"Batch {i+1}: eye {left_eye.shape}, facial {facial_features.shape}, gaze {gaze_point.shape}\")\n","            assert not torch.isnan(left_eye).any(), \"NaN in left_eye\"\n","            assert not torch.isinf(left_eye).any(), \"Inf in left_eye\"\n","            assert not torch.isnan(facial_features).any(), \"NaN in facial_features\"\n","            assert not torch.isinf(facial_features).any(), \"Inf in facial_features\"\n","            assert (gaze_point >= 0).all() and (gaze_point <= 1).all(), \"Gaze point out of [0,1] range\"\n","\n","            eye_means.append(left_eye.mean().item())\n","            eye_stds.append(left_eye.std().item())\n","            feat_means.append(facial_features.mean().item())\n","            feat_stds.append(facial_features.std().item())\n","\n","            if i + 1 >= num_batches:\n","                break\n","\n","        print(f\"Eye mean ~ {np.mean(eye_means):.4f}, std ~ {np.mean(eye_stds):.4f}\")\n","        print(f\"Feat mean ~ {np.mean(feat_means):.4f}, std ~ {np.mean(feat_stds):.4f}\")\n","        print(\"Verification passed âœ…\")\n","\n","    # --- your plotting function ---\n","    def plot_samples(self, split=\"train\", num_samples=5):\n","        if split == \"train\":\n","            dataset = self.train_dataset\n","        elif split == \"val\":\n","            dataset = self.val_dataset\n","        elif split == \"test\":\n","            dataset = self.test_dataset\n","        else:\n","            raise ValueError(\"split must be 'train', 'val', or 'test'\")\n","\n","        indices = random.sample(range(len(dataset)), num_samples)\n","\n","        fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n","        gaze_points = []\n","\n","        for i, idx in enumerate(indices):\n","            left_eye, _, gaze_point = dataset[idx]\n","            img = left_eye.permute(1, 2, 0).numpy()\n","            img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n","            axes[i].imshow(img)\n","            axes[i].axis(\"off\")\n","            axes[i].set_title(f\"Sample {i+1}\")\n","            gaze_points.append(gaze_point.numpy())\n","\n","        plt.show()\n","\n","        gaze_points = np.array(gaze_points)\n","        plt.figure(figsize=(5, 5))\n","        plt.scatter(gaze_points[:, 0], gaze_points[:, 1], c=\"red\", marker=\"x\", s=80)\n","        plt.xlim(0, 1)\n","        plt.ylim(0, 1)\n","        plt.gca().invert_yaxis()\n","        plt.xlabel(\"Normalized X\")\n","        plt.ylabel(\"Normalized Y\")\n","        plt.title(f\"Gaze points ({split} set)\")\n","        plt.grid(True, linestyle=\"--\", alpha=0.6)\n","        plt.show()\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","        )\n","\n","    def test_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","        )"],"metadata":{"id":"HbM3TNLZ_yAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"n1NuLkXyCPGk"},"execution_count":null,"outputs":[]}]}